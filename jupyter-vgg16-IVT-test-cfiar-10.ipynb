{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "This script trains a head for the Xception network to predict on CFAIR 10\n",
    "after training this head the weights for the head are realigned into the dogs and cats dataset\n",
    "and are trained for 1 epoch the test results of the head transfered and realigned are tested against a random initialization\n",
    "head's test results and the directly transfered head.\n",
    "\n",
    "so\n",
    "1. train head on CIFAR -\n",
    "2. transfer traditionally to cats and dogs dataset -\n",
    "3. transfer using IVT - debugging stage\n",
    "4. use random init head\n",
    "\n",
    "currently train is using 7,000 samples and test - 3,000 samples\n",
    "'''\n",
    "\n",
    "# imports\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data\n",
    "!pip install pytorchcv --quiet\n",
    "from pytorchcv.model_provider import get_model\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000004\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "i = np.ones([1, 500])/500\n",
    "print(i.sum())\n",
    "o = i.dot(np.ones([500, 10])/10)\n",
    "print(o.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# universal functions:\n",
    "class Head(torch.nn.Module):\n",
    "\tdef __init__(self, in_f, out_f):\n",
    "\t\tsuper(Head, self).__init__()\n",
    "\t\t#self.avgp = nn.AdaptiveAvgPool2d(1)\n",
    "\t\tself.f = nn.Flatten()\n",
    "\t\tself.b1 = nn.BatchNorm1d(in_f)\n",
    "\t\tself.r = nn.ReLU()\n",
    "\t\tself.o = nn.Linear(in_f, out_f)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t#x = self.avgp(x)\n",
    "\t\tx = self.f(x)\n",
    "\t\tx = self.r(x)\n",
    "\t\tx = self.b1(x)\n",
    "\t\tout = self.o(x)\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class FCN(torch.nn.Module):\n",
    "\tdef __init__(self, base, in_f):\n",
    "\t\tsuper(FCN, self).__init__()\n",
    "\t\tself.base = base\n",
    "\t\tself.h1 = Head(in_f, 10)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.base(x)\n",
    "\t\treturn self.h1(x)\n",
    "\n",
    "\n",
    "def test_acc(testloaderc, modelt, width=32):\n",
    "\tif width == 32:\n",
    "\t\tacc = 0\n",
    "\t\ttotal = 0\n",
    "\t\tfor (x, y) in testloaderc:\n",
    "\t\t\tx = x.reshape([x.shape[0], 3, width, width]).cuda()\n",
    "\t\t\tpred = modelt(x)\n",
    "\t\t\tfor c, i in enumerate(pred):\n",
    "\t\t\t\tif torch.argmax(i) == y[c]:\n",
    "\t\t\t\t\tacc += 1\n",
    "\t\t\t\ttotal += 1\n",
    "\t\treturn acc / total\n",
    "\telse:\n",
    "\t\tacc = 0\n",
    "\t\ttotal = 0\n",
    "\t\tfor (x, y) in testloaderc:\n",
    "\t\t\tx = x.reshape([x.shape[0], 3, width, width]).cuda()\n",
    "\t\t\tpred = modelt(x)\n",
    "\t\t\tfor c, i in enumerate(pred):\n",
    "\t\t\t\tif y[c] == 1 and i >= .5:\n",
    "\t\t\t\t\tacc += 1\n",
    "\t\t\t\tif y[c] == 0 and i <= .4:\n",
    "\t\t\t\t\tacc += 1\n",
    "\t\t\t\ttotal += 1\n",
    "\t\treturn acc / total\n",
    "\n",
    "\n",
    "def train(trainloaderc, modelc, epochs, testloader, optimizerc, width=32):\n",
    "\tmodelc = modelc.cuda()\n",
    "\tif width == 32:\n",
    "\t\tcriterion = nn.CrossEntropyLoss()\n",
    "\telse:\n",
    "\t\tcriterion = torch.nn.functional.binary_cross_entropy_with_logits\n",
    "\n",
    "\ttotal_loss = 0\n",
    "\tt = trainloaderc #tqdm(trainloaderc)\n",
    "\tfor _ in range(epochs):\n",
    "\n",
    "\t\tfor i, (x, y) in enumerate(t):\n",
    "\t\t\tbs = x.shape[0]\n",
    "\t\t\tif width == 32:\n",
    "\t\t\t\ty = y.type(torch.LongTensor).cuda()\n",
    "\t\t\tx = x.reshape([x.shape[0], 3, width, width]).cuda()\n",
    "\n",
    "\t\t\tpred = modelc(x)\n",
    "\n",
    "\t\t\t#print(pred.shape)\n",
    "\t\t\t#print(y.shape)\n",
    "\t\t\tif width != 32:\n",
    "\t\t\t\tpred = pred.reshape([bs]).cuda()  #.type(torch.LongTensor)\n",
    "\t\t\t#print(pred.shape)\n",
    "\t\t\t#print(type(pred))\n",
    "\t\t\tloss = criterion(pred.cpu(), y.cpu())\n",
    "\t\t\toptimizerc.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizerc.step()\n",
    "\t\t\ttotal_loss += loss.cpu().detach().numpy()\n",
    "\t\t\t#t.set_description(f'loss: %.4f' % (total_loss / (i + 1)))\n",
    "\treturn modelc\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------- ivt model ---------------------------------\n",
    "\n",
    "# --------------------------------- ivt model ---------------------------------\n",
    "\n",
    "def ivt_realign(model, trainloader, l):\n",
    "\n",
    "    intermediate = torch.tensor([])\n",
    "    layers_up_to = []\n",
    "\n",
    "\n",
    "    layers = [module for module in model.pre.modules() if type(module) != nn.Sequential]\n",
    "    print(layers)\n",
    "    for __, layer in enumerate(layers):\n",
    "        if 'ConvBlock' in str(layer):\n",
    "            #print(layer)\n",
    "            layer = list(layer.children())[0]\n",
    "            #print(layer)\n",
    "        \n",
    "        if 'Norm' in str(layer):\n",
    "            #print('found')\n",
    "            #layers_up_to.append(layer)\n",
    "            #print(layer.weight.shape)\n",
    "            continue\n",
    "        \n",
    "        if 'Lin' in str(layer):\n",
    "            print(str(layer))\n",
    "            for ii, (X2, Y2) in enumerate(trainloader):\n",
    "                # could increase compuatational efficiency by looping through data as most outside loop and loop through\n",
    "                # layers in the innermost, do this for now tho, this could be better/more accurate as to what i am doing\n",
    "\n",
    "                batch_size = X2.shape[0] # data\n",
    "                pre = X2.reshape([batch_size, 3, 32, 32]).cuda()\n",
    "\n",
    "                for up_to in layers_up_to: # get upto this point in calculation\n",
    "                        #print(pre.shape)\n",
    "                        pre = up_to(pre)\n",
    "\n",
    "\n",
    "                #btw indexes needed\n",
    "                pre = pre.mean(axis=0)/pre.mean(axis=0).sum() # w valuues\n",
    "                #print(layer.weight.shape)\n",
    "                w = layer.weight.shape[0] # assume w is 3072\n",
    "                h = layer.weight.shape[1]\n",
    "                #print(layer.weight.shape)\n",
    "                #print(pre.shape)\n",
    "                \n",
    "                new = np.array([])\n",
    "                for i in layer.weight.sum(axis=1): # loop through h times\n",
    "                    #i = 1/(layer.weight.shape[1]**2) # remove in transfer learning -------------------------------------------------------------------------------------------\n",
    "                    new = np.append(new, (i * pre).cpu().detach().numpy())\n",
    "\n",
    "               \n",
    "                new = new.reshape([w, h]) # this should work\n",
    "                #print(new[0, 0, :, :])\n",
    "                intermediate = torch.cat([intermediate.float().cuda(), torch.tensor(new).reshape([1, w, h]).float().cuda()]) # or whatever size\n",
    "                \n",
    "                #break\n",
    "            \n",
    "            #print(intermediate.mean(axis=0).shape)\n",
    "            intermediate = intermediate.mean(axis=0)\n",
    "            layer.weight = torch.nn.Parameter(intermediate, requires_grad=True)\n",
    "            #x = intermediate.mean(axis=0).cpu().detach().numpy()\n",
    "            #x = Image.fromarray(x, 'RGB')\n",
    "            #x.save(str(__) + '2.png')\n",
    "            intermediate = torch.tensor([])\n",
    "            layers_up_to.append(layer)\n",
    "            continue\n",
    "            \n",
    "        if 'nv' not in str(layer):\n",
    "            layers_up_to.append(layer)\n",
    "            continue\n",
    "\n",
    "        print(\"cur layer: \", layer)\n",
    "        #print(__)\n",
    "\n",
    "        false_set = False\n",
    "\n",
    "        for ii, (X2, Y2) in enumerate(trainloader):\n",
    "            # could increase compuatational efficiency by looping through data as most outside loop and loop through\n",
    "            # layers in the innermost, do this for now tho, this could be better/more accurate as to what i am doing\n",
    "\n",
    "            batch_size = X2.shape[0] # data\n",
    "            pre = X2.reshape([batch_size, 3, l, l]).cuda()\n",
    "\n",
    "            for up_to in layers_up_to: # get upto this point in calculation\n",
    "                    pre = up_to(pre)\n",
    "\n",
    "\n",
    "            #btw indexes needed\n",
    "            pre = pre.mean(axis=0)\n",
    "            #print(layer.weight.shape)\n",
    "            #print(pre.shape)\n",
    "            k = layer.weight.shape[2] # k size\n",
    "            features = pre.shape[0] # features in input\n",
    "            print(pre.shape)\n",
    "            img_size = pre.shape[2]\n",
    "            weight_sums_size = layer.weight.shape[0]\n",
    "            weight_sums = torch.tensor([]).cuda()\n",
    "\n",
    "            try:\n",
    "                lwr = layer.weight.reshape([features, weight_sums_size, k * k])\n",
    "            except:\n",
    "                false_set = True\n",
    "                print('identified')\n",
    "                break\n",
    "\n",
    "            for _, i in enumerate(lwr):\n",
    "                #print(i.shape)\n",
    "                if _ != 0:\n",
    "                    weight_sums = torch.cat([weight_sums, i.sum(axis=1).reshape([1, weight_sums_size])])\n",
    "                else:\n",
    "                    weight_sums = i.sum(axis=1).reshape([1, weight_sums_size])\n",
    "\n",
    "            #print(weight_sums.shape) # should be 3x64\n",
    "\n",
    "            imp_vals = torch.tensor([])\n",
    "            for i in range(features):\n",
    "\n",
    "                #print(pre.shape)\n",
    "                try:\n",
    "                    f = np.array(pre[i, :, :].cpu()).reshape([img_size, img_size])# do 3x3 convolution on this\n",
    "                except:\n",
    "                    f = np.array(pre[i, :, :].cpu().detach()).reshape([img_size, img_size])  # do 3x3 convolution on this\n",
    "\n",
    "                f = np.pad(f, pad_width=1, mode='constant', constant_values=1) # pad with 1s bc vgg16 uses 1 padding for conv                \n",
    "                view_shape = tuple(np.subtract(f.shape, (k, k)) + 1) + (k, k)\n",
    "                strides = f.strides + f.strides\n",
    "\n",
    "                sub_matrices = np.lib.stride_tricks.as_strided(f, view_shape, strides)\n",
    "                #print(sub_matrices.shape)\n",
    "\n",
    "                this_imp = torch.tensor(sub_matrices.reshape([k*k, sub_matrices.shape[0]*sub_matrices.shape[1]]).mean(axis=1)).cuda().reshape([1, k, k])# importance for 3x3 conv weight\n",
    "\n",
    "                #given filter is 1.\n",
    "                #conv_resultT = torch.tensor(sub_matrices.reshape([9, sub_matrices.shape[0]*sub_matrices.shape[1]])).sum(axis=0) # results in input shape?\n",
    "                #print(conv_resultT)\n",
    "\n",
    "                if i != 0:\n",
    "                    imp_vals = torch.cat([imp_vals, torch.tensor(np.nan_to_num((this_imp/this_imp.sum()).cpu().detach().numpy()).reshape([1, k, k])).cuda()])\n",
    "                else:\n",
    "                    imp_vals = torch.tensor(np.nan_to_num((this_imp/this_imp.sum()).cpu().detach().numpy()).reshape([1, k, k])).cuda()\n",
    "            #print(imp_vals[0]) # should be 3x3x3\n",
    "            #print(imp_vals.shape)\n",
    "\n",
    "            # go through in k sizes and find each position's imp_Val/sum_div value\n",
    "            # sum feature dims together\n",
    "            # imp vals are overall feature dims in input, one imp val of size kxk\n",
    "            new = torch.tensor([]).cuda()\n",
    "            for feature in range(features): # use kernel of given feature dim and sum and redistribute\n",
    "                for i in weight_sums[feature]:\n",
    "                    new = torch.cat([new.cuda(), (i * imp_vals[feature]).reshape([1, k, k]).cuda()]) # multiply sum by channel's importancce matrix\n",
    "            new = new.reshape([weight_sums_size, features, k, k]) # this should work\n",
    "            #print(new[0, 0, :, :])\n",
    "            intermediate = torch.cat([intermediate.cuda(), torch.tensor(new).reshape([1, weight_sums_size, features, k, k]).cuda()]) # or whatever size\n",
    "            #break\n",
    "\n",
    "        if false_set:\n",
    "            continue\n",
    "        \n",
    "        layer.weight = torch.nn.Parameter(intermediate.mean(axis=0), requires_grad=True)\n",
    "        intermediate = torch.tensor([])\n",
    "        layers_up_to.append(layer)\n",
    "        #x = layer(X2.reshape([batch_size, 3, l, l]).cuda()).mean(axis=0).cpu().detach().numpy()\n",
    "        #x = x.reshape([x.shape[1], x.shape[1], x.shape[2]])\n",
    "        #print(x.shape)\n",
    "        #x = Image.fromarray(x, 'RGB')\n",
    "        #x.save(str(__) + 'f.png')\n",
    "        \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convmodel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(convmodel, self).__init__()\n",
    "        \n",
    "        self.pre = get_model(\"vgg16\", pretrained=True)\n",
    "        self.pre = nn.Sequential(*list(self.pre.children())[:-1])  # Remove original output layer\n",
    "        self.pre.eval()\n",
    "        \n",
    "        #self.conv1 = nn.Conv2d(3, 3, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "        #self.b1 = nn.BatchNorm2d([3])#  nn.BatchNorm2d([3, 33, 33])\n",
    "        \n",
    "        #self.conv2 = nn.Conv2d(3, 3, kernel_size=(3, 3), stride=(3, 3), padding=(1, 1)) #nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        #self.b2 = nn.BatchNorm2d([3]) #nn.InstanceNorm2d([32 ,32]) nn.BatchNorm1d\n",
    "        \n",
    "        self.f = nn.Flatten()\n",
    "        \n",
    "        self.conv3 = nn.Linear(512, 10)#nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.b3 = nn.BatchNorm1d([10])#nn.InstanceNorm1d([32 ,32])\n",
    "        \n",
    "        #self.b1.weight = torch.nn.Parameter(torch.ones([3]))\n",
    "        #self.b2.weight = torch.nn.Parameter(torch.ones([3]))\n",
    "        self.b3.weight = torch.nn.Parameter(torch.ones([10]))\n",
    "        \n",
    "        #self.b1.bias = torch.nn.Parameter(torch.ones([3]))\n",
    "        #self.b2.bias = torch.nn.Parameter(torch.ones([3]))\n",
    "        self.b3.bias = torch.nn.Parameter(torch.ones([10]))\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = self.conv1(x)\n",
    "        #x = self.relu(self.b1(x))\n",
    "        #x = self.conv2(x)\n",
    "        #x = self.relu(self.b2(x))\n",
    "        x = self.pre(x)\n",
    "        x = self.f(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(self.b3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to CFIAR/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146fa2eeb18543feb4cd0dde13d9cd3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting CFIAR/cifar-10-python.tar.gz to CFIAR\n",
      "Files already downloaded and verified\n",
      "(50000, 32, 32, 3)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------------------- get data ---------------------------------------------\n",
    "\n",
    "trainset = datasets.CIFAR10(root='CFIAR', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root='CFIAR', train=False, download=True, transform=transform)\n",
    "\n",
    "print(trainset.data.shape)\n",
    "\n",
    "\n",
    "trainset = data.TensorDataset(torch.Tensor(trainset.data), torch.Tensor(trainset.targets))\n",
    "testset = data.TensorDataset(torch.Tensor(testset.data), torch.Tensor(testset.targets))\n",
    "\n",
    "trainloaderl = torch.utils.data.DataLoader(trainset, batch_size=5000, shuffle=True, num_workers=0)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "del trainset\n",
    "del testset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /root/.torch/models/vgg16-0865-5ca155da.pth.zip from https://github.com/osmr/imgclsmob/releases/download/v0.0.401/vgg16-0865-5ca155da.pth.zip...\n",
      "[ConvBlock(\n",
      "  (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activ): ReLU(inplace=True)\n",
      "), Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), ConvBlock(\n",
      "  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activ): ReLU(inplace=True)\n",
      "), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), ConvBlock(\n",
      "  (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activ): ReLU(inplace=True)\n",
      "), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), ConvBlock(\n",
      "  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activ): ReLU(inplace=True)\n",
      "), Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), ConvBlock(\n",
      "  (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activ): ReLU(inplace=True)\n",
      "), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), ConvBlock(\n",
      "  (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activ): ReLU(inplace=True)\n",
      "), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), ConvBlock(\n",
      "  (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activ): ReLU(inplace=True)\n",
      "), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), ConvBlock(\n",
      "  (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activ): ReLU(inplace=True)\n",
      "), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), ConvBlock(\n",
      "  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activ): ReLU(inplace=True)\n",
      "), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), ConvBlock(\n",
      "  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activ): ReLU(inplace=True)\n",
      "), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), ConvBlock(\n",
      "  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activ): ReLU(inplace=True)\n",
      "), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), ConvBlock(\n",
      "  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activ): ReLU(inplace=True)\n",
      "), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), ConvBlock(\n",
      "  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activ): ReLU(inplace=True)\n",
      "), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)]\n",
      "cur layer:  Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([3, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "torch.Size([3, 32, 32])\n",
      "torch.Size([3, 32, 32])\n",
      "torch.Size([3, 32, 32])\n",
      "torch.Size([3, 32, 32])\n",
      "torch.Size([3, 32, 32])\n",
      "torch.Size([3, 32, 32])\n",
      "torch.Size([3, 32, 32])\n",
      "torch.Size([3, 32, 32])\n",
      "cur layer:  Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([64, 32, 32])\n",
      "identified\n",
      "cur layer:  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([64, 32, 32])\n",
      "torch.Size([64, 32, 32])\n",
      "torch.Size([64, 32, 32])\n",
      "torch.Size([64, 32, 32])\n",
      "torch.Size([64, 32, 32])\n",
      "torch.Size([64, 32, 32])\n",
      "torch.Size([64, 32, 32])\n",
      "torch.Size([64, 32, 32])\n",
      "torch.Size([64, 32, 32])\n",
      "torch.Size([64, 32, 32])\n",
      "cur layer:  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([64, 32, 32])\n",
      "torch.Size([64, 32, 32])\n",
      "torch.Size([64, 32, 32])\n",
      "torch.Size([64, 32, 32])\n",
      "torch.Size([64, 32, 32])\n",
      "torch.Size([64, 32, 32])\n",
      "torch.Size([64, 32, 32])\n",
      "torch.Size([64, 32, 32])\n",
      "torch.Size([64, 32, 32])\n",
      "torch.Size([64, 32, 32])\n",
      "cur layer:  Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([64, 16, 16])\n",
      "torch.Size([64, 16, 16])\n",
      "torch.Size([64, 16, 16])\n",
      "torch.Size([64, 16, 16])\n",
      "torch.Size([64, 16, 16])\n",
      "torch.Size([64, 16, 16])\n",
      "torch.Size([64, 16, 16])\n",
      "torch.Size([64, 16, 16])\n",
      "torch.Size([64, 16, 16])\n",
      "torch.Size([64, 16, 16])\n",
      "cur layer:  Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([128, 16, 16])\n",
      "identified\n",
      "cur layer:  Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([128, 16, 16])\n",
      "torch.Size([128, 16, 16])\n",
      "torch.Size([128, 16, 16])\n",
      "torch.Size([128, 16, 16])\n",
      "torch.Size([128, 16, 16])\n",
      "torch.Size([128, 16, 16])\n",
      "torch.Size([128, 16, 16])\n",
      "torch.Size([128, 16, 16])\n",
      "torch.Size([128, 16, 16])\n",
      "torch.Size([128, 16, 16])\n",
      "cur layer:  Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([128, 16, 16])\n",
      "torch.Size([128, 16, 16])\n",
      "torch.Size([128, 16, 16])\n",
      "torch.Size([128, 16, 16])\n",
      "torch.Size([128, 16, 16])\n",
      "torch.Size([128, 16, 16])\n",
      "torch.Size([128, 16, 16])\n",
      "torch.Size([128, 16, 16])\n",
      "torch.Size([128, 16, 16])\n",
      "torch.Size([128, 16, 16])\n",
      "cur layer:  Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([128, 8, 8])\n",
      "torch.Size([128, 8, 8])\n",
      "torch.Size([128, 8, 8])\n",
      "torch.Size([128, 8, 8])\n",
      "torch.Size([128, 8, 8])\n",
      "torch.Size([128, 8, 8])\n",
      "torch.Size([128, 8, 8])\n",
      "torch.Size([128, 8, 8])\n",
      "torch.Size([128, 8, 8])\n",
      "torch.Size([128, 8, 8])\n",
      "cur layer:  Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([256, 8, 8])\n",
      "identified\n",
      "cur layer:  Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "cur layer:  Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "cur layer:  Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "cur layer:  Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "torch.Size([256, 8, 8])\n",
      "cur layer:  Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([256, 4, 4])\n",
      "torch.Size([256, 4, 4])\n",
      "torch.Size([256, 4, 4])\n",
      "torch.Size([256, 4, 4])\n",
      "torch.Size([256, 4, 4])\n",
      "torch.Size([256, 4, 4])\n",
      "torch.Size([256, 4, 4])\n",
      "torch.Size([256, 4, 4])\n",
      "torch.Size([256, 4, 4])\n",
      "torch.Size([256, 4, 4])\n",
      "cur layer:  Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([512, 4, 4])\n",
      "identified\n",
      "cur layer:  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "cur layer:  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "cur layer:  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "cur layer:  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "torch.Size([512, 4, 4])\n",
      "cur layer:  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "cur layer:  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "cur layer:  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "cur layer:  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "cur layer:  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "cur layer:  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 2])\n",
      "accuracy CIFAR:  0.2229\n",
      "accuracy CIFAR:  0.2596\n",
      "accuracy CIFAR:  0.3132\n",
      "accuracy CIFAR:  0.3207\n",
      "accuracy CIFAR:  0.3334\n",
      "accuracy CIFAR:  0.3475\n",
      "accuracy CIFAR:  0.3721\n",
      "accuracy CIFAR:  0.3801\n",
      "accuracy CIFAR:  0.4073\n",
      "accuracy CIFAR:  0.4124\n",
      "accuracy CIFAR:  0.4062\n",
      "accuracy CIFAR:  0.4299\n",
      "accuracy CIFAR:  0.4356\n",
      "accuracy CIFAR:  0.4489\n",
      "accuracy CIFAR:  0.4495\n",
      "accuracy CIFAR:  0.4609\n",
      "accuracy CIFAR:  0.4673\n",
      "accuracy CIFAR:  0.4724\n",
      "accuracy CIFAR:  0.4816\n",
      "accuracy CIFAR:  0.4928\n",
      "accuracy CIFAR:  0.4913\n",
      "accuracy CIFAR:  0.4944\n",
      "accuracy CIFAR:  0.4965\n",
      "accuracy CIFAR:  0.5059\n",
      "accuracy CIFAR:  0.5019\n",
      "accuracy CIFAR:  0.5007\n",
      "accuracy CIFAR:  0.4978\n",
      "accuracy CIFAR:  0.5004\n",
      "accuracy CIFAR:  0.4886\n",
      "accuracy CIFAR:  0.4957\n",
      "0.5059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------- ivt model ----------------------------------\n",
    "model1 = convmodel().cuda()#\n",
    "for param in model1.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "optimizer = torch.optim.AdamW(model1.parameters(), lr=0.001)\n",
    "model1 = ivt_realign(model1.cuda(), trainloaderl, 32)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model1.parameters(), lr=0.001)\n",
    "acc = []\n",
    "\n",
    "for skee_yee in range(epochs):\n",
    "\tmodel1 = train(trainloader, model1, 1, testloader, optimizer)\n",
    "\tacc.append(test_acc(testloader, model1))\n",
    "\tprint(\"accuracy CIFAR: \", acc[skee_yee])\n",
    "\n",
    "print(np.array(acc).max())\n",
    "    \n",
    "del optimizer\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy CIFAR:  0.3933\n",
      "accuracy CIFAR:  0.477\n",
      "accuracy CIFAR:  0.518\n",
      "accuracy CIFAR:  0.5531\n",
      "accuracy CIFAR:  0.5738\n",
      "accuracy CIFAR:  0.5699\n",
      "accuracy CIFAR:  0.5843\n",
      "accuracy CIFAR:  0.5953\n",
      "accuracy CIFAR:  0.5955\n",
      "accuracy CIFAR:  0.5976\n",
      "accuracy CIFAR:  0.5846\n",
      "accuracy CIFAR:  0.5965\n",
      "accuracy CIFAR:  0.5906\n",
      "accuracy CIFAR:  0.5892\n",
      "accuracy CIFAR:  0.5866\n",
      "accuracy CIFAR:  0.5825\n",
      "accuracy CIFAR:  0.5853\n",
      "accuracy CIFAR:  0.58\n",
      "accuracy CIFAR:  0.585\n",
      "accuracy CIFAR:  0.586\n",
      "accuracy CIFAR:  0.5787\n",
      "accuracy CIFAR:  0.5834\n",
      "accuracy CIFAR:  0.5743\n",
      "accuracy CIFAR:  0.5775\n",
      "accuracy CIFAR:  0.5836\n",
      "accuracy CIFAR:  0.5826\n",
      "accuracy CIFAR:  0.585\n",
      "accuracy CIFAR:  0.5807\n",
      "accuracy CIFAR:  0.5781\n",
      "accuracy CIFAR:  0.5775\n",
      "accuracy CIFAR:  0.5854\n",
      "0.5976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------- ivt model ----------------------------------\n",
    "model2 = convmodel().cuda()#get_model(\"vgg16\", pretrained=True) # convmodel()\n",
    "for param in model2.parameters():\n",
    "    param.requires_grad = True\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model2.parameters(), lr=0.001)\n",
    "acc2 = []\n",
    "\n",
    "for skee_yee in range(31):\n",
    "\tmodel2 = train(trainloader, model2, 1, testloader, optimizer)\n",
    "\tacc2.append(test_acc(testloader, model2))\n",
    "\tprint(\"accuracy CIFAR: \", acc2[skee_yee])\n",
    "\n",
    "print(np.array(acc2).max())\n",
    "    \n",
    "\n",
    "del optimizer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc, label='IVT-I acc')\n",
    "plt.plot(acc2, label='normal acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "146fa2eeb18543feb4cd0dde13d9cd3e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5ccc1f5961974a34b9011faf7a0fb833",
        "IPY_MODEL_601c27dd43fa4928ab9a59816934893b"
       ],
       "layout": "IPY_MODEL_833a5d2fa4c74f10afc5ae7885e500e4"
      }
     },
     "41fc875c157e42a097503af902882c89": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5ccc1f5961974a34b9011faf7a0fb833": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_41fc875c157e42a097503af902882c89",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c241538ba53847338c2f2a7e2c57b73d",
       "value": 1
      }
     },
     "601c27dd43fa4928ab9a59816934893b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7eec688fe92d438ea56de63809be6833",
       "placeholder": "​",
       "style": "IPY_MODEL_c8917f405b304714991757f8a53c30cd",
       "value": " 170500096/? [00:09&lt;00:00, 18462637.02it/s]"
      }
     },
     "7eec688fe92d438ea56de63809be6833": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "833a5d2fa4c74f10afc5ae7885e500e4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c241538ba53847338c2f2a7e2c57b73d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "c8917f405b304714991757f8a53c30cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
